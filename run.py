import os
import re
import sys
import json
import xml.etree.ElementTree as ET
import platform
import tempfile
import requests
import shutil
import subprocess
from datetime import datetime, timedelta
import pytz


def write_json(path, data, encoding="utf8"):
    """å†™å…¥json"""
    with open(path, "w", encoding=encoding) as f:
        json.dump(data, f, ensure_ascii=False, indent=4)


def read_json(path, default_data={}, encoding="utf8"):
    """è¯»å–json"""
    data = {}
    if os.path.exists(path):
        try:
            data = json.loads(open(path, "r", encoding=encoding).read())
        except:
            data = default_data
            write_json(path, data, encoding=encoding)

    else:
        data = default_data
        write_json(path, data, encoding=encoding)
    return data

def get_executable_path():
    '''è·å–å¯æ‰§è¡Œæ–‡ä»¶è·¯å¾„'''
    system = platform.system()
    if system == 'Windows':
        executable_path = './bin/wechatmp2markdown-v1.1.11_win64.exe'
    else:
        executable_path = './bin/wechatmp2markdown-v1.1.11_linux_amd64'
    # æ·»åŠ æ‰§è¡Œæƒé™
    os.chmod(executable_path, 0o755)
    # è¿”å›å¯æ‰§è¡Œæ–‡ä»¶çš„å®Œæ•´è·¯å¾„
    return executable_path

def get_md_path(executable_path,url):
    '''è·å–mdæ–‡ä»¶è·¯å¾„'''
    temp_directory = tempfile.mkdtemp()
    command = [executable_path, url, temp_directory, '--image=url']
    subprocess.check_output(command)
    for root, _, files in os.walk(temp_directory):
        for file in files:
            if file.endswith(".md"):
                file_path = os.path.join(root, file)
                yield file_path

def get_chainreactors_url():
    '''è·å–ä»Šæ—¥url'''
    current_date = datetime.now(pytz.timezone('Asia/Shanghai')).strftime("%Y-%m-%d")
    base_url = 'https://raw.githubusercontent.com/chainreactors/picker/refs/heads/master/archive/daily/{}/{}.md'.format(current_date[:4], current_date)
    headers = {
        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
        'accept-language': 'zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6',
        'cache-control': 'no-cache',
        'pragma': 'no-cache',
        'priority': 'u=0, i',
        'referer': 'https://github.com/chainreactors/picker',
        'sec-ch-ua': '"Chromium";v="130", "Microsoft Edge";v="130", "Not?A_Brand";v="99"',
        'sec-ch-ua-mobile': '?0',
        'sec-ch-ua-platform': '"Windows"',
        'sec-fetch-dest': 'document',
        'sec-fetch-mode': 'navigate',
        'sec-fetch-site': 'cross-site',
        'sec-fetch-user': '?1',
        'upgrade-insecure-requests': '1',
        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36 Edg/130.0.0.0',
    }
    try:
        response = requests.get(
            base_url,
            headers=headers,
        )
        urls = re.findall('(?:å¤ç°|æ¼æ´|CVE-\d+|CNVD-\d+|CNNVD-\d+|XVE-\d+|QVD-\d+|POC|EXP|0day|1day|nday|RCE|ä»£ç æ‰§è¡Œ|å‘½ä»¤æ‰§è¡Œ).*?(https://mp.weixin.qq.com/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*]|(?:%[0-9a-fA-F][0-9a-fA-F]))+)',response.text,re.I)
        urls = [url.rstrip(')') for url in urls]
        return urls
    except:
        return []

def get_BruceFeIix_url():
    '''è·å–ä»Šæ—¥url'''
    current_date = datetime.now(pytz.timezone('Asia/Shanghai')).strftime("%Y-%m-%d")
    base_url = 'https://raw.githubusercontent.com/BruceFeIix/picker/refs/heads/master/archive/daily/{}/{}.md'.format(current_date[:4], current_date)
    headers = {
        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
        'accept-language': 'zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6',
        'cache-control': 'no-cache',
        'pragma': 'no-cache',
        'priority': 'u=0, i',
        'referer': 'https://github.com/BruceFeIix/picker',
        'sec-ch-ua': '"Chromium";v="130", "Microsoft Edge";v="130", "Not?A_Brand";v="99"',
        'sec-ch-ua-mobile': '?0',
        'sec-ch-ua-platform': '"Windows"',
        'sec-fetch-dest': 'document',
        'sec-fetch-mode': 'navigate',
        'sec-fetch-site': 'cross-site',
        'sec-fetch-user': '?1',
        'upgrade-insecure-requests': '1',
        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36 Edg/130.0.0.0',
    }
    try:
        response = requests.get(
            base_url,
            headers=headers,
        )
        urls = re.findall('(?:å¤ç°|æ¼æ´|CVE-\d+|CNVD-\d+|CNNVD-\d+|XVE-\d+|QVD-\d+|POC|EXP|0day|1day|nday|RCE|ä»£ç æ‰§è¡Œ|å‘½ä»¤æ‰§è¡Œ).*?(https://mp.weixin.qq.com/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*]|(?:%[0-9a-fA-F][0-9a-fA-F]))+)',response.text,re.I)
        urls = [url.rstrip(')') for url in urls]
        return urls
    except:
        return []

import xml.etree.ElementTree as ET

def get_doonsec_url():
    '''ä» Doonsec RSS è·å–ä»Šæ—¥URLï¼Œä½¿ç”¨ XML è§£æ'''
    cookies = {
        'UM_follow': 'True',
        'UM_distinctids': 'fgmr',
        'session': 'eyJfcGVybWFuZW50Ijp0cnVlLCJjc3JmX3Rva2VuIjoiMzU2ZDE4OTcwZjliZDljY2NjN2M3YzlkMzRhOGVlZWQyZDk1NmI1ZSIsInZpc3RvciI6ImZHTXJGQXBlVndRUnZrWjJHdWplV2gifQ.ZzidRw.GyjS15N12JYU0TByO31rrwBIiPY',
    }

    headers = {
        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
        'accept-language': 'zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6',
        'cache-control': 'no-cache',
        'pragma': 'no-cache',
        'priority': 'u=0, i',
        'sec-ch-ua': '"Chromium";v="130", "Microsoft Edge";v="130", "Not?A_Brand";v="99"',
        'sec-ch-ua-mobile': '?0',
        'sec-ch-ua-platform': '"Windows"',
        'sec-fetch-dest': 'document',
        'sec-fetch-mode': 'navigate',
        'sec-fetch-site': 'same-origin',
        'sec-fetch-user': '?1',
        'upgrade-insecure-requests': '1',
        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36 Edg/130.0.0.0',
    }

    try:
        response = requests.get('https://wechat.doonsec.com/rss.xml', cookies=cookies, headers=headers)
        response.encoding = response.apparent_encoding

        # XML è§£æ
        root = ET.fromstring(response.text)
        urls = []
        for item in root.findall('./channel/item'):
            title = item.findtext('title') or ''
            link = item.findtext('link') or ''
            if re.search(r'(å¤ç°|æ¼æ´|CVE-\d+|CNVD-\d+|CNNVD-\d+|XVE-\d+|QVD-\d+|POC|EXP|0day|1day|nday|RCE|ä»£ç æ‰§è¡Œ|å‘½ä»¤æ‰§è¡Œ)', title, re.I) and link.startswith('https://mp.weixin.qq.com/'):
                urls.append(link.rstrip(')'))

        return urls
    except Exception as e:
        print("Error parsing Doonsec RSS:", e)
        return []


def get_issue_url():
    file = '/tmp/issue_content.txt'
    if os.path.exists(file):
        content = open(file,'r',encoding='utf8').read()
        urls = re.findall('(https://mp.weixin.qq.com/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*]|(?:%[0-9a-fA-F][0-9a-fA-F]))+)',content,re.I)
        urls = [url.rstrip(')') for url in urls]
        return urls
    return []
    
def rep_filename(result_path):
    ''' 
    æ›¿æ¢ä¸èƒ½ç”¨äºæ–‡ä»¶åçš„å­—ç¬¦
    '''
    for root, _, files in os.walk(result_path):
        for file in files:
            if file.endswith(".md"):
                file_path = os.path.join(root, file)
                new_file = re.sub(r'[\/\\\:\*\?\"\<\>\|]', '', file)
                shutil.move(os.path.join(root, file), os.path.join(root, new_file))
                
def update_readme(urls):
    """æ›´æ–°README.mdæ–‡ä»¶"""
    today = get_beijing_time().strftime('%Y-%m-%d')
    
    # è¯»å–ç°æœ‰å†…å®¹
    with open('README.md', 'r', encoding='utf-8') as f:
        content = f.read()
    
    # å‡†å¤‡æ–°å†…å®¹
    new_content = f"""## ğŸ“¢ {today}æ—¥æ–°å¢æ–‡ç« 

"""
    
    # è·å–æ–‡ç« ä¿¡æ¯
    articles = []
    data = read_json('data.json')
    for url in urls:
        if url in data:
            title = data[url]
            articles.append({'title': title, 'url': url})
    
    # æ·»åŠ æ–‡ç« 
    for i, article in enumerate(articles, 1):
        new_content += f"{i}. {article['title']} ğŸ”—[æ¥æº]({article['url']})\n\n"
    
    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
    new_content += f"""#### ğŸ“Š ç»Ÿè®¡ä¿¡æ¯
<small>ğŸ“ æ–°å¢æ–‡ç« æ•°ï¼š{len(articles)}ç¯‡
â° æ›´æ–°æ—¶é—´ï¼š{get_beijing_time().strftime('%Y-%m-%d %H:%M:%S')}<small>

---
"""
    
    # åœ¨æ›´æ–°æ—¥å¿—éƒ¨åˆ†æ’å…¥æ–°å†…å®¹
    update_log_marker = "## ğŸ“ æ›´æ–°æ—¥å¿—"
    update_log_index = content.find(update_log_marker)
    
    if update_log_index != -1:
        # åœ¨æ›´æ–°æ—¥å¿—æ ‡é¢˜åæ’å…¥æ–°å†…å®¹
        new_content = content[:update_log_index + len(update_log_marker)] + "\n\n" + new_content + content[update_log_index + len(update_log_marker):]
    else:
        # å¦‚æœæ‰¾ä¸åˆ°æ›´æ–°æ—¥å¿—éƒ¨åˆ†ï¼Œåœ¨æ–‡ä»¶æœ«å°¾æ·»åŠ 
        new_content = content + "\n\n" + new_content
    
    # å†™å…¥æ›´æ–°åçš„å†…å®¹
    with open('README.md', 'w', encoding='utf-8') as f:
        f.write(new_content)
                
def get_beijing_time():
    return datetime.now(pytz.timezone('Asia/Shanghai'))

def archive_readme():
    """æ¯å‘¨å½’æ¡£README.mdæ–‡ä»¶"""
    current_time = get_beijing_time()
    # è·å–å½“å‰æ˜¯ç¬¬å‡ å‘¨
    week_number = current_time.isocalendar()[1]
    year = current_time.year
    
    # å¦‚æœæ˜¯å‘¨æ—¥ï¼ˆisoweekday()è¿”å›7è¡¨ç¤ºå‘¨æ—¥ï¼‰
    if current_time.isoweekday() == 7:
        archive_dir = 'archives'
        os.makedirs(archive_dir, exist_ok=True)
        
        # è®¡ç®—æœ¬å‘¨ä¸€çš„æ—¥æœŸ
        monday = current_time - timedelta(days=current_time.isoweekday()-1)
        
        # åˆ›å»ºå½’æ¡£æ–‡ä»¶åï¼Œæ ¼å¼ï¼šREADME-YYYY-MM-DD_DD.md
        archive_filename = f'README-{monday.strftime("%Y-%m-%d")}_{current_time.strftime("%d")}.md'
        archive_path = os.path.join(archive_dir, archive_filename)
        
        # å¦‚æœREADME.mdå­˜åœ¨ï¼Œåˆ™è¿›è¡Œå½’æ¡£
        if os.path.exists('README.md'):
            # å¤åˆ¶å½“å‰README.mdåˆ°å½’æ¡£æ–‡ä»¶
            shutil.copy2('README.md', archive_path)
            
            # åˆ›å»ºæ–°çš„README.mdï¼Œä¿ç•™åŸºæœ¬ç»“æ„
            new_readme_content = f"""# å¾®ä¿¡å…¬ä¼—å·å®‰å…¨æ¼æ´æ–‡ç« å½’æ¡£

[![GitHub Actions](https://github.com/gelusus/wxvl/actions/workflows/update_today.yml/badge.svg)](https://github.com/gelusus/wxvl/actions)
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)

é¡¹ç›®Forkè‡ª[gelusus](https://github.com/gelusus/wxvl)

## âœ¨ é¡¹ç›®åŠŸèƒ½

è‡ªåŠ¨æŠ“å–å¾®ä¿¡å…¬ä¼—å·å®‰å…¨æ¼æ´æ–‡ç« ï¼Œè½¬æ¢ä¸ºMarkdownæ ¼å¼å¹¶å»ºç«‹æœ¬åœ°çŸ¥è¯†åº“ï¼Œæ¯æ—¥æŒç»­æ›´æ–°ï¼Œå¹¶åŒæ­¥åˆ°é’‰é’‰ç¾¤ã€‚

## ğŸ“Š æœ¬å‘¨ç»Ÿè®¡
- å¼€å§‹æ—¶é—´ï¼š{current_time.strftime('%Y-%m-%d')}
- æ–‡ç« æ€»æ•°ï¼š0 ç¯‡
- æœ€åæ›´æ–°ï¼š{current_time.strftime('%Y-%m-%d %H:%M:%S')}

## ğŸ“ æ›´æ–°æ—¥å¿—

---
"""
            with open('README.md', 'w', encoding='utf-8') as f:
                f.write(new_readme_content)
            
            print(f"å·²å½’æ¡£æœ¬å‘¨README.mdåˆ° {archive_filename}")

def main():
    '''ä¸»å‡½æ•°'''
    # æ£€æŸ¥æ˜¯å¦éœ€è¦å½’æ¡£
    archive_readme()
    
    data_file = 'data.json'
    data = {}
    executable_path = get_executable_path()
    base_result_path = 'doc'
    # åˆ›å»ºåŸºäºå½“å‰å¹´æœˆçš„å­ç›®å½• (æ ¼å¼: YYYY-MM)
    current_month = datetime.now(pytz.timezone('Asia/Shanghai')).strftime("%Y-%m")
    result_path = os.path.join(base_result_path, current_month)
    os.makedirs(result_path, exist_ok=True)
    # è¯»å–å†å²è®°å½•
    data = read_json(data_file, default_data=data)
    if len(sys.argv) == 2:
        if sys.argv[1] == 'today':
            urls = list(set(get_chainreactors_url() + get_BruceFeIix_url() + get_doonsec_url()))
        else:
            urls = get_issue_url()
        
        new_urls = []  # è®°å½•æ–°æ·»åŠ çš„URL
        for url in urls:
            if url in data:
                continue
            for file_path in get_md_path(executable_path, url):
                name = os.path.splitext(os.path.basename(file_path))[0]
                if name == '.md':
                    continue
                shutil.copy2(file_path,result_path)
                data[url] = name
                write_json(data_file,data)
                new_urls.append(url)  # æ·»åŠ åˆ°æ–°URLåˆ—è¡¨
                print(name,end='ã€')
        
        # å¦‚æœæœ‰æ–°æ–‡ç« ï¼Œæ›´æ–°README.md
        if new_urls:
            update_readme(new_urls)
    
    rep_filename(result_path)
if __name__ == '__main__':
    main()

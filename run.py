import os
import re
import sys
import json
import xml.etree.ElementTree as ET
import platform
import tempfile
import requests
import shutil
import subprocess
from datetime import datetime


def write_json(path, data, encoding="utf8"):
    """å†™å…¥json"""
    with open(path, "w", encoding=encoding) as f:
        json.dump(data, f, ensure_ascii=False, indent=4)


def read_json(path, default_data={}, encoding="utf8"):
    """è¯»å–json"""
    data = {}
    if os.path.exists(path):
        try:
            data = json.loads(open(path, "r", encoding=encoding).read())
        except:
            data = default_data
            write_json(path, data, encoding=encoding)

    else:
        data = default_data
        write_json(path, data, encoding=encoding)
    return data

def get_executable_path():
    '''è·å–å¯æ‰§è¡Œæ–‡ä»¶è·¯å¾„'''
    system = platform.system()
    if system == 'Windows':
        executable_path = './bin/wechatmp2markdown-v1.1.11_win64.exe'
    else:
        executable_path = './bin/wechatmp2markdown-v1.1.11_linux_amd64'
    # æ·»åŠ æ‰§è¡Œæƒé™
    os.chmod(executable_path, 0o755)
    # è¿”å›å¯æ‰§è¡Œæ–‡ä»¶çš„å®Œæ•´è·¯å¾„
    return executable_path

def get_md_path(executable_path,url):
    '''è·å–mdæ–‡ä»¶è·¯å¾„'''
    temp_directory = tempfile.mkdtemp()
    command = [executable_path, url, temp_directory, '--image=url']
    subprocess.check_output(command)
    for root, _, files in os.walk(temp_directory):
        for file in files:
            if file.endswith(".md"):
                file_path = os.path.join(root, file)
                yield file_path

def get_chainreactors_url():
    '''è·å–ä»Šæ—¥url'''
    current_date = datetime.now().strftime("%Y-%m-%d")
    base_url = 'https://raw.githubusercontent.com/chainreactors/picker/refs/heads/master/archive/daily/{}/{}.md'.format(current_date[:4], current_date)
    headers = {
        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
        'accept-language': 'zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6',
        'cache-control': 'no-cache',
        'pragma': 'no-cache',
        'priority': 'u=0, i',
        'referer': 'https://github.com/chainreactors/picker',
        'sec-ch-ua': '"Chromium";v="130", "Microsoft Edge";v="130", "Not?A_Brand";v="99"',
        'sec-ch-ua-mobile': '?0',
        'sec-ch-ua-platform': '"Windows"',
        'sec-fetch-dest': 'document',
        'sec-fetch-mode': 'navigate',
        'sec-fetch-site': 'cross-site',
        'sec-fetch-user': '?1',
        'upgrade-insecure-requests': '1',
        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36 Edg/130.0.0.0',
    }
    try:
        response = requests.get(
            base_url,
            headers=headers,
        )
        urls = re.findall('(?:å¤ç°|æ¼æ´|CVE-\d+|CNVD-\d+|CNNVD-\d+|XVE-\d+|QVD-\d+|POC|EXP|0day|1day|nday|RCE|ä»£ç æ‰§è¡Œ|å‘½ä»¤æ‰§è¡Œ).*?(https://mp.weixin.qq.com/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*]|(?:%[0-9a-fA-F][0-9a-fA-F]))+)',response.text,re.I)
        urls = [url.rstrip(')') for url in urls]
        return urls
    except:
        return []

def get_BruceFeIix_url():
    '''è·å–ä»Šæ—¥url'''
    current_date = datetime.now().strftime("%Y-%m-%d")
    base_url = 'https://raw.githubusercontent.com/BruceFeIix/picker/refs/heads/master/archive/daily/{}/{}.md'.format(current_date[:4], current_date)
    headers = {
        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
        'accept-language': 'zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6',
        'cache-control': 'no-cache',
        'pragma': 'no-cache',
        'priority': 'u=0, i',
        'referer': 'https://github.com/BruceFeIix/picker',
        'sec-ch-ua': '"Chromium";v="130", "Microsoft Edge";v="130", "Not?A_Brand";v="99"',
        'sec-ch-ua-mobile': '?0',
        'sec-ch-ua-platform': '"Windows"',
        'sec-fetch-dest': 'document',
        'sec-fetch-mode': 'navigate',
        'sec-fetch-site': 'cross-site',
        'sec-fetch-user': '?1',
        'upgrade-insecure-requests': '1',
        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36 Edg/130.0.0.0',
    }
    try:
        response = requests.get(
            base_url,
            headers=headers,
        )
        urls = re.findall('(?:å¤ç°|æ¼æ´|CVE-\d+|CNVD-\d+|CNNVD-\d+|XVE-\d+|QVD-\d+|POC|EXP|0day|1day|nday|RCE|ä»£ç æ‰§è¡Œ|å‘½ä»¤æ‰§è¡Œ).*?(https://mp.weixin.qq.com/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*]|(?:%[0-9a-fA-F][0-9a-fA-F]))+)',response.text,re.I)
        urls = [url.rstrip(')') for url in urls]
        return urls
    except:
        return []

import xml.etree.ElementTree as ET

def get_doonsec_url():
    '''ä» Doonsec RSS è·å–ä»Šæ—¥URLï¼Œä½¿ç”¨ XML è§£æ'''
    cookies = {
        'UM_follow': 'True',
        'UM_distinctids': 'fgmr',
        'session': 'eyJfcGVybWFuZW50Ijp0cnVlLCJjc3JmX3Rva2VuIjoiMzU2ZDE4OTcwZjliZDljY2NjN2M3YzlkMzRhOGVlZWQyZDk1NmI1ZSIsInZpc3RvciI6ImZHTXJGQXBlVndRUnZrWjJHdWplV2gifQ.ZzidRw.GyjS15N12JYU0TByO31rrwBIiPY',
    }

    headers = {
        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
        'accept-language': 'zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6',
        'cache-control': 'no-cache',
        'pragma': 'no-cache',
        'priority': 'u=0, i',
        'sec-ch-ua': '"Chromium";v="130", "Microsoft Edge";v="130", "Not?A_Brand";v="99"',
        'sec-ch-ua-mobile': '?0',
        'sec-ch-ua-platform': '"Windows"',
        'sec-fetch-dest': 'document',
        'sec-fetch-mode': 'navigate',
        'sec-fetch-site': 'same-origin',
        'sec-fetch-user': '?1',
        'upgrade-insecure-requests': '1',
        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36 Edg/130.0.0.0',
    }

    try:
        response = requests.get('https://wechat.doonsec.com/rss.xml', cookies=cookies, headers=headers)
        response.encoding = response.apparent_encoding

        # XML è§£æ
        root = ET.fromstring(response.text)
        urls = []
        for item in root.findall('./channel/item'):
            title = item.findtext('title') or ''
            link = item.findtext('link') or ''
            if re.search(r'(å¤ç°|æ¼æ´|CVE-\d+|CNVD-\d+|CNNVD-\d+|XVE-\d+|QVD-\d+|POC|EXP|0day|1day|nday|RCE|ä»£ç æ‰§è¡Œ|å‘½ä»¤æ‰§è¡Œ)', title, re.I) and link.startswith('https://mp.weixin.qq.com/'):
                urls.append(link.rstrip(')'))

        return urls
    except Exception as e:
        print("Error parsing Doonsec RSS:", e)
        return []


def get_issue_url():
    file = '/tmp/issue_content.txt'
    if os.path.exists(file):
        content = open(file,'r',encoding='utf8').read()
        urls = re.findall('(https://mp.weixin.qq.com/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*]|(?:%[0-9a-fA-F][0-9a-fA-F]))+)',content,re.I)
        urls = [url.rstrip(')') for url in urls]
        return urls
    return []
    
def rep_filename(result_path):
    ''' 
    æ›¿æ¢ä¸èƒ½ç”¨äºæ–‡ä»¶åçš„å­—ç¬¦
    '''
    for root, _, files in os.walk(result_path):
        for file in files:
            if file.endswith(".md"):
                file_path = os.path.join(root, file)
                new_file = re.sub(r'[\/\\\:\*\?\"\<\>\|]', '', file)
                shutil.move(os.path.join(root, file), os.path.join(root, new_file))
                
def update_readme(urls):
    """æ›´æ–°README.mdæ–‡ä»¶"""
    today = datetime.now().strftime('%Y-%m-%d')
    
    # è¯»å–ç°æœ‰å†…å®¹
    with open('README.md', 'r', encoding='utf-8') as f:
        content = f.read()
    
    # å‡†å¤‡æ–°å†…å®¹
    new_content = f"""## ğŸ“¢ datetime.now().strftime('%Y-%m-%d')æ—¥æ›´æ–°æé†’

### ğŸ“š æ–°å¢æ–‡ç« 

"""
    
    # è·å–æ–‡ç« ä¿¡æ¯
    articles = []
    data = read_json('data.json')
    for url in urls:
        if url in data:
            title = data[url]
            articles.append({'title': title, 'url': url})
    
    # æ·»åŠ æ–‡ç« 
    for i, article in enumerate(articles, 1):
        new_content += f"{i}. {article['title']} <sub>ğŸ”—[æ¥æº]({article['url']})</sub> \n\n"
    
    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
    new_content += f"""#### ğŸ“Š ç»Ÿè®¡ä¿¡æ¯

<sub>ğŸ“ æ–°å¢æ–‡ç« æ•°ï¼š{len(articles)}ç¯‡</sub>  
<sub>â° æ›´æ–°æ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</sub>

---
"""
    
    # åœ¨æ›´æ–°æ—¥å¿—éƒ¨åˆ†æ’å…¥æ–°å†…å®¹
    update_log_marker = "## ğŸ“ æ›´æ–°æ—¥å¿—"
    update_log_index = content.find(update_log_marker)
    
    if update_log_index != -1:
        # åœ¨æ›´æ–°æ—¥å¿—æ ‡é¢˜åæ’å…¥æ–°å†…å®¹
        new_content = content[:update_log_index + len(update_log_marker)] + "\n\n" + new_content + content[update_log_index + len(update_log_marker):]
    else:
        # å¦‚æœæ‰¾ä¸åˆ°æ›´æ–°æ—¥å¿—éƒ¨åˆ†ï¼Œåœ¨æ–‡ä»¶æœ«å°¾æ·»åŠ 
        new_content = content + "\n\n" + new_content
    
    # å†™å…¥æ›´æ–°åçš„å†…å®¹
    with open('README.md', 'w', encoding='utf-8') as f:
        f.write(new_content)
                
def main():
    '''ä¸»å‡½æ•°'''
    data_file = 'data.json'
    data = {}
    executable_path = get_executable_path()
    base_result_path = 'doc'
    # åˆ›å»ºåŸºäºå½“å‰å¹´æœˆçš„å­ç›®å½• (æ ¼å¼: YYYY-MM)
    current_month = datetime.now().strftime("%Y-%m")
    result_path = os.path.join(base_result_path, current_month)
    os.makedirs(result_path, exist_ok=True)
    # è¯»å–å†å²è®°å½•
    data = read_json(data_file, default_data=data)
    if len(sys.argv) == 2:
        if sys.argv[1] == 'today':
            urls = list(set(get_chainreactors_url() + get_BruceFeIix_url() + get_doonsec_url()))
        else:
            urls = get_issue_url()
        
        new_urls = []  # è®°å½•æ–°æ·»åŠ çš„URL
        for url in urls:
            if url in data:
                continue
            for file_path in get_md_path(executable_path, url):
                name = os.path.splitext(os.path.basename(file_path))[0]
                if name == '.md':
                    continue
                shutil.copy2(file_path,result_path)
                data[url] = name
                write_json(data_file,data)
                new_urls.append(url)  # æ·»åŠ åˆ°æ–°URLåˆ—è¡¨
                print(name,end='ã€')
        
        # å¦‚æœæœ‰æ–°æ–‡ç« ï¼Œæ›´æ–°README.md
        if new_urls:
            update_readme(new_urls)
    
    rep_filename(result_path)
if __name__ == '__main__':
    main()
